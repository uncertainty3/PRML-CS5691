{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbcaef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "176676a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91990\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7a04a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91990\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68fc6df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(fname):\n",
    "    rows = []\n",
    "    tfile = tarfile.open(fname, 'r:gz')\n",
    "    for member in tfile.getmembers():\n",
    "        if 'ham' in member.name:\n",
    "            f = tfile.extractfile(member)\n",
    "            if f is not None:\n",
    "                row = f.read()\n",
    "                rows.append({'message': row, 'class': 'ham'})\n",
    "        if 'spam' in member.name:\n",
    "            f = tfile.extractfile(member)\n",
    "            if f is not None:\n",
    "                row = f.read()\n",
    "                rows.append({'message': row, 'class': 'spam'})\n",
    "    tfile.close()\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57f4b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = extractor(\"C:\\\\Users\\\\91990\\\\Downloads\\\\enron1.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "655c1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = extractor(\"C:\\\\Users\\\\91990\\\\Downloads\\\\enron2.tar.gz\")\n",
    "#df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a2723b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.iloc[5171][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe24e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.iloc[5000][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab82479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['class'] = (df['class'] == 'spam')\n",
    "#df['class'] = df['class']*1\n",
    "#df['class'] = df['class'].astype(int)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "776970cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test['class'] = (df_test['class'] == 'spam')\n",
    "#df['class'] = df['class']*1\n",
    "#df['class'] = df['class'].astype(int)\n",
    "#df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc8c73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['class'] = df['class']*1\n",
    "#df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5e7713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test['class'] = df_test['class']*1\n",
    "#df_test['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "572f80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['message'] = df['message'].apply(lambda x: x.decode('latin-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70c06faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test['message'] = df_test['message'].apply(lambda x: x.decode('latin-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88fb10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.reset_index(drop=True)\n",
    "#df.iloc[5171].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bbc669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = df_test.reset_index(drop=True)\n",
    "#df_test.iloc[5171].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcd21f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def clean_email(email):\n",
    "    email = re.sub(r'http\\S+', 'link ', email)\n",
    "    email = re.sub(\"\\d+\", \" \", email)\n",
    "    email = re.sub('[$]+', ' dollar ', email)\n",
    "    #email = re.sub('www+', ' link ', email)\n",
    "    #email = re.sub('http+', ' link ', email)\n",
    "    email = email.replace('\\n', ' ')\n",
    "    email = email.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "    email = email.lower()\n",
    "    return email\n",
    "\n",
    "#df['message'] = df['message'].apply(clean_email)\n",
    "#df_test['message'] = df_test['message'].apply(clean_email)\n",
    "\n",
    "#df.iloc[5171].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7492fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def Lemmatize(email):\n",
    "    tokens = word_tokenize(email)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed = []\n",
    "    for word in tokens:\n",
    "        word = re.compile('[^a-zA-Z0-9]').sub('', word).strip()\n",
    "        if len(word)>2:\n",
    "            processed.append(lemmatizer.lemmatize(word))\n",
    "    return \" \".join(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82aeb751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['message'] = df['message'].apply(Lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a7e594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test['message'] = df_test['message'].apply(Lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "961619d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.iloc[5171].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42f413b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X = df['message'].values\n",
    "#y = df['class'].values\n",
    "#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "#X = vectorizer.fit_transform(X)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, shuffle=True, random_state=0, stratify=y_final)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0, stratify=y)\n",
    "#print(X[1])\n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28dd285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_names = vectorizer.get_feature_names_out()\n",
    "#print(\"Number of different words: {0}\".format(len(feature_names)))\n",
    "#print(\"Word example: {0}\".format(feature_names[38299]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d08d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train.shape, y_train.shape)\n",
    "#print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba1912f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.naive_bayes import BernoulliNB\\nclf = BernoulliNB()\\nclf.fit(X_train, y_train)\\npred = clf.predict(X_test)\\nprint(\"Accuracy: {}\".format(clf.score(X_test, y_test)))'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a8ab8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_most_important_features(vectorizer, classifier, n=None):\\n    feature_names = vectorizer.get_feature_names_out()\\n    top_features = sorted(zip(classifier.feature_log_prob_[0], feature_names))[-n:]\\n    for coef, feat in top_features:\\n        print(coef, feat)\\n\\nget_most_important_features(vectorizer, clf, 0)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def get_most_important_features(vectorizer, classifier, n=None):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_features = sorted(zip(classifier.feature_log_prob_[0], feature_names))[-n:]\n",
    "    for coef, feat in top_features:\n",
    "        print(coef, feat)\n",
    "\n",
    "get_most_important_features(vectorizer, clf, 0)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7580dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'email = [\"spam\"]\\nexamples = vectorizer.transform(email)\\npredictions = clf.predict(examples)\\npredictions'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"email = [\"spam\"]\n",
    "examples = vectorizer.transform(email)\n",
    "predictions = clf.predict(examples)\n",
    "predictions\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f09e652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\\nimport numpy as np\\ndef feature_extraction(Mails):\\n\\n    CV = CountVectorizer()\\n    term_frequency_vector = CV.fit_transform(Mails)\\n\\n    print(f\"All unique words found: {term_frequency_vector.shape[1]}\")\\n    # print(len(Mails))\\n\\n    tfidf = TfidfTransformer()\\n    tfidf.fit(term_frequency_vector)\\n\\n    df_idf = pd.DataFrame(tfidf.idf_, index = CV.get_feature_names(),columns=[\"idf\"]) \\n    \\n    orderedWords = df_idf.sort_values(by=[\\'idf\\'])  # Sort in increasing order\\n\\n    print(\"Extracting and removing Stop-Words\")\\n    stopwords = [orderedWords[\\'idf\\'].index[i] for i in range(40)]  # Top 40 words with the lowest IDF weight have been used as stopwords as they do no provide any discrimination across mails\\n    print(stopwords)\\n\\n    number_of_features = 2500\\n\\n    # VOCAB_LIMIT = 2500 #number of words to be included in the dictionary apart from the stop words\\n    CV2 = CountVectorizer(stop_words = stopwords, max_features = number_of_features)\\n    term_frequency_vector2 = CV2.fit_transform(Mails)\\n    print(term_frequency_vector2.shape)\\n\\n    length = len(CV2.vocabulary_)\\n    print(f\"Number of words after stop words removal:{length}\")\\n    \\n    #pickle.dump(CV2, open(\"dict2500_rtf.pkl\", \"wb\"))    # storing dictionary which contains a total of VOCAB_LIMIT words, excluding the stop words\\n\\n\\n    \\n    Top_features = np.empty((0,length),int)\\n    i=0\\n    for email in Mails:\\n        print(i)\\n        i=i+1\\n        Top_features=np.append(Top_features, CV2.transform([email]).toarray(),axis=0)\\n\\n    print(f\"Got {Top_features.shape} train features\")\\n\\n    return Top_features, CV2.vocabulary_'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import numpy as np\n",
    "def feature_extraction(Mails):\n",
    "\n",
    "    CV = CountVectorizer()\n",
    "    term_frequency_vector = CV.fit_transform(Mails)\n",
    "\n",
    "    print(f\"All unique words found: {term_frequency_vector.shape[1]}\")\n",
    "    # print(len(Mails))\n",
    "\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf.fit(term_frequency_vector)\n",
    "\n",
    "    df_idf = pd.DataFrame(tfidf.idf_, index = CV.get_feature_names(),columns=[\"idf\"]) \n",
    "    \n",
    "    orderedWords = df_idf.sort_values(by=['idf'])  # Sort in increasing order\n",
    "\n",
    "    print(\"Extracting and removing Stop-Words\")\n",
    "    stopwords = [orderedWords['idf'].index[i] for i in range(40)]  # Top 40 words with the lowest IDF weight have been used as stopwords as they do no provide any discrimination across mails\n",
    "    print(stopwords)\n",
    "\n",
    "    number_of_features = 2500\n",
    "\n",
    "    # VOCAB_LIMIT = 2500 #number of words to be included in the dictionary apart from the stop words\n",
    "    CV2 = CountVectorizer(stop_words = stopwords, max_features = number_of_features)\n",
    "    term_frequency_vector2 = CV2.fit_transform(Mails)\n",
    "    print(term_frequency_vector2.shape)\n",
    "\n",
    "    length = len(CV2.vocabulary_)\n",
    "    print(f\"Number of words after stop words removal:{length}\")\n",
    "    \n",
    "    #pickle.dump(CV2, open(\"dict2500_rtf.pkl\", \"wb\"))    # storing dictionary which contains a total of VOCAB_LIMIT words, excluding the stop words\n",
    "\n",
    "\n",
    "    \"\"\"\"\"\"\n",
    "    Top_features = np.empty((0,length),int)\n",
    "    i=0\n",
    "    for email in Mails:\n",
    "        print(i)\n",
    "        i=i+1\n",
    "        Top_features=np.append(Top_features, CV2.transform([email]).toarray(),axis=0)\n",
    "\n",
    "    print(f\"Got {Top_features.shape} train features\")\n",
    "\n",
    "    return Top_features, CV2.vocabulary_\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a43d937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(path):\n",
    "    df = extractor(path)\n",
    "    df['class'] = (df['class'] == 'spam')\n",
    "    df['class'] = df['class']*1\n",
    "    df['message'] = df['message'].apply(lambda x: x.decode('latin-1'))\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['message'] = df['message'].apply(clean_email)\n",
    "    df['message'] = df['message'].apply(Lemmatize)\n",
    "    l = list(df['message'])\n",
    "    y = df['class'].values\n",
    "    return l, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0665ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l, y = processing(\"C:\\\\Users\\\\91990\\\\Downloads\\\\enron1.tar.gz\")\n",
    "l1, y1 = processing(\"C:\\\\Users\\\\91990\\\\Downloads\\\\enron2.tar.gz\")\n",
    "l2, y2 = processing(\"C:\\\\Users\\\\91990\\\\Downloads\\\\enron3.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c17686bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "L = l + l1 + l2\n",
    "Y = list(y) + list(y1) + list(y2)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84e7bca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[5171]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9430350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l = list(df['message'])\n",
    "#l[5171]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc06d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l_test = list(df_test['message'])\n",
    "#l_test[5171]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3757cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_vec, features = feature_extraction(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "724f9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34b7a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "593530dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"To_remove = [] \\nfor i in range(f_vec.shape[0]):\\n    if np.sum(f_vec[i,:])==0:\\n        To_remove.append(i)\\n\\nX_final = np.delete(f_vec, To_remove, axis=0)    # Removing samples with 0 frequencies of all chosen words\\ny_final = np.delete(y,To_remove)\\nprint(X_final.shape)\\nprint(y_final.shape)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"To_remove = [] \n",
    "for i in range(f_vec.shape[0]):\n",
    "    if np.sum(f_vec[i,:])==0:\n",
    "        To_remove.append(i)\n",
    "\n",
    "X_final = np.delete(f_vec, To_remove, axis=0)    # Removing samples with 0 frequencies of all chosen words\n",
    "y_final = np.delete(y,To_remove)\n",
    "print(X_final.shape)\n",
    "print(y_final.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb36d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(f_vec[0])):\n",
    " #   if f_vec[0][i] > 0:\n",
    "  #      print(i)\n",
    "   #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a2f195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#value = list(features)[398]\n",
    "#value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9439f182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16541x83080 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1691451 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "san = TfidfVectorizer()\n",
    "san.fit_transform(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67587379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1.221830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>1.345887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1.377125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>1.433232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magsecurities</th>\n",
       "      <td>10.020511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maha</th>\n",
       "      <td>10.020511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mahaffey</th>\n",
       "      <td>10.020511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mahal</th>\n",
       "      <td>10.020511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>10.020511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83080 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     idf\n",
       "subject         1.000000\n",
       "the             1.221830\n",
       "for             1.345887\n",
       "and             1.377125\n",
       "you             1.433232\n",
       "...                  ...\n",
       "magsecurities  10.020511\n",
       "maha           10.020511\n",
       "mahaffey       10.020511\n",
       "mahal          10.020511\n",
       "aa             10.020511\n",
       "\n",
       "[83080 rows x 1 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_san = pd.DataFrame(san.idf_, index = san.get_feature_names_out(),columns=[\"idf\"]) \n",
    "orderedWords = df_san.sort_values(by=['idf'])\n",
    "orderedWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dff11343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject', 'the', 'for', 'and', 'you', 'this', 'have', 'with', 'from', 'that', 'are', 'will', 'your', 'please', 'not', 'enron', 'can', 'our', 'any', 'all']\n"
     ]
    }
   ],
   "source": [
    "stopwords = [orderedWords['idf'].index[i] for i in range(20)] \n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c712443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#san_2 = TfidfVectorizer(stop_words = stopwords, max_features = 3000)\n",
    "#san_2.fit_transform(l).toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2302c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7afb00d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abb</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abuse</th>\n",
       "      <th>academic</th>\n",
       "      <th>accept</th>\n",
       "      <th>...</th>\n",
       "      <th>yet</th>\n",
       "      <th>yield</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhiyong</th>\n",
       "      <th>zimin</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16536</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16537</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16538</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16539</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16540</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16541 rows × 4000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abb  ability  able  about  above  absence  absolutely  abuse  academic  \\\n",
       "0        0        0     0      0      0        0           0      0         0   \n",
       "1        0        0     0      1      0        0           0      0         0   \n",
       "2        0        0     0      0      0        0           0      0         0   \n",
       "3        0        0     0      0      0        0           0      0         0   \n",
       "4        0        0     0      0      0        0           0      0         0   \n",
       "...    ...      ...   ...    ...    ...      ...         ...    ...       ...   \n",
       "16536    0        0     0      0      0        0           0      0         0   \n",
       "16537    0        0     1      0      0        0           0      0         1   \n",
       "16538    0        0     0      0      0        0           0      0         0   \n",
       "16539    0        0     0      0      0        0           0      0         0   \n",
       "16540    0        0     0      0      0        0           0      0         0   \n",
       "\n",
       "       accept  ...  yet  yield  york  young  yours  yourself  zero  zhiyong  \\\n",
       "0           0  ...    0      0     0      0      0         0     0        0   \n",
       "1           0  ...    0      0     0      0      0         0     0        0   \n",
       "2           0  ...    0      0     0      0      0         0     0        0   \n",
       "3           0  ...    0      0     0      0      0         0     0        0   \n",
       "4           0  ...    0      0     0      0      0         0     0        0   \n",
       "...       ...  ...  ...    ...   ...    ...    ...       ...   ...      ...   \n",
       "16536       0  ...    0      0     0      0      0         0     0        0   \n",
       "16537       0  ...    0      0     0      0      0         0     0        0   \n",
       "16538       0  ...    0      0     0      0      0         0     0        0   \n",
       "16539       0  ...    0      0     0      0      0         0     0        0   \n",
       "16540       0  ...    0      0     0      0      0         0     0        0   \n",
       "\n",
       "       zimin  zone  \n",
       "0          0     0  \n",
       "1          0     0  \n",
       "2          0     0  \n",
       "3          0     0  \n",
       "4          0     0  \n",
       "...      ...   ...  \n",
       "16536      0     0  \n",
       "16537      0     0  \n",
       "16538      0     0  \n",
       "16539      0     0  \n",
       "16540      0     0  \n",
       "\n",
       "[16541 rows x 4000 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "san_3 = CountVectorizer(stop_words = stopwords, max_features = 4000)\n",
    "v_san = san_3.fit_transform(L)\n",
    "#print(v_san[1])\n",
    "#san_3.get_feature_names_out()\n",
    "count_array = v_san.toarray()\n",
    "df = pd.DataFrame(data=count_array,columns = san_3.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "194e56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "To_remove = [] \n",
    "for i in range(count_array.shape[0]):\n",
    "    if np.sum(count_array[i,:])==0:\n",
    "        To_remove.append(i)\n",
    "X_real1 = np.delete(count_array, To_remove, axis=0)\n",
    "y_real1 = np.delete(Y,To_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "787a42fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_real1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f97c62a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9669797031202666\n"
     ]
    }
   ],
   "source": [
    "#X_real = ar_train.toarray()\n",
    "#y_real = y\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_real1, y_real1, test_size=0.2, shuffle=True, random_state=None, stratify=y_real1)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39403d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13200, 4000)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0389be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_san = san_2.fit_transform(l).toarray()\n",
    "#y_san = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b3c27648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_san[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea1e87d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train, X_test, y_train, y_test = train_test_split(X_san, y_san, test_size=0.2, shuffle=True, random_state=0, stratify=y_san)\\nfrom sklearn.naive_bayes import BernoulliNB\\nclf = BernoulliNB()\\nclf.fit(X_train, y_train)\\npred = clf.predict(X_test)\\nprint(\"Accuracy: {}\".format(clf.score(X_test, y_test)))'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"X_train, X_test, y_train, y_test = train_test_split(X_san, y_san, test_size=0.2, shuffle=True, random_state=0, stratify=y_san)\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "962eaef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB(X_train, y_train, X_test):\n",
    "    spam_ind = np.where(y_train == 1)[0]\n",
    "    ham_ind = np.where(y_train == 0)[0]\n",
    "    tot = len(y_train)\n",
    "    spam = len(spam_ind)\n",
    "    ham = len(ham_ind)\n",
    "    p_spam = spam/tot\n",
    "    p_ham = ham/tot\n",
    "\n",
    "    spam_mails = X_train[spam_ind]\n",
    "    r = np.ones(shape = (1, X_train.shape[1]))\n",
    "    spam_mails = np.append(spam_mails, r , axis=0)\n",
    "    ham_mails = X_train[ham_ind]\n",
    "    ham_mails = np.append(ham_mails, r , axis=0)\n",
    "    spam_words = np.zeros(X_train.shape[1])\n",
    "    ham_words = np.zeros(X_train.shape[1])\n",
    "\n",
    "    for mail in spam_mails:\n",
    "        spam_words = spam_words + mail\n",
    "    for mail in ham_mails:\n",
    "        ham_words = ham_words + mail\n",
    "\n",
    "    tot_spam_words = np.sum(spam_words)\n",
    "    tot_ham_words = np.sum(ham_words)\n",
    "\n",
    "    p_spam_word = []\n",
    "    p_ham_word = []\n",
    "\n",
    "    for i in range(spam_mails.shape[1]):\n",
    "        s = 0\n",
    "        for j in range(spam_mails.shape[0]):\n",
    "            s = s + spam_mails[j][i]\n",
    "        s = s/tot_spam_words\n",
    "        p_spam_word.append(s)\n",
    "\n",
    "    for i in range(ham_mails.shape[1]):\n",
    "        s = 0\n",
    "        for j in range(ham_mails.shape[0]):\n",
    "            s = s + ham_mails[j][i]\n",
    "        s = s/tot_ham_words\n",
    "        p_ham_word.append(s)\n",
    "\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        p_test_spam = 0\n",
    "        p_test_ham = 0\n",
    "        for j in range(X_test.shape[1]):\n",
    "            if(X_test[i,j]!=0):\n",
    "                #if(p_spam_word[j]!=0):\n",
    "                p_test_spam += np.log(p_spam_word[j])\n",
    "\n",
    "                #if(p_ham_word[j]!=0):\n",
    "                p_test_ham += np.log(p_ham_word[j])\n",
    "            #else:\n",
    "                #p_test_spam += np.log((1-p_spam_word[j])*p_spam)\n",
    "                #p_test_ham += np.log((1-p_ham_word[j])*p_ham)\n",
    "        p_test_spam += np.log(p_spam)\n",
    "        p_test_ham += np.log(p_ham)\n",
    "        if(p_test_ham > p_test_spam):\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "\n",
    "    return y_pred\n",
    "    #len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "578abfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3301"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_NB = NB(X_train, y_train, X_test)\n",
    "len(pred_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "942e75ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3301"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69c14f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(orig, pred):\n",
    "    return (1 - (np.sum(abs(orig-pred)))/len(pred))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc05d287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.66767646167827"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_NB = accuracy(y_test, pred_NB)\n",
    "acc_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "deb00022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9251741896395032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce1eae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test, y_true = processing(\"C:\\\\Users\\\\91990\\\\Downloads\\\\enron4.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22463e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.75"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#g = [\"click this link to join the group chat\"]\n",
    "v = san_3.transform(l_test)\n",
    "v = v.toarray()\n",
    "p = NB(X_real1, y_real1, v)\n",
    "a = accuracy(y_true, p)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43564c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 4000)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6c91fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.5"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_real1, y_real1)\n",
    "pred = clf.predict(v)\n",
    "pr = accuracy(y_true, pred)\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "56ae8db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitting\n",
      "0\n",
      "Model fitting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4824/3806116332.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#pickle.dump(modelSVM, f)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0my_pred_svm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelSVM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_svm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\91990\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    808\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\91990\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\91990\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_dense_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[0msvm_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLIBSVM_IMPL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         return libsvm.predict(\n\u001b[0m\u001b[0;32m    455\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "a = []\n",
    "for i in range(10):\n",
    "    modelSVM=svm.SVC(C=5)\n",
    "    print(\"Model fitting\")\n",
    "    modelSVM.fit(X_real1, y_real1)\n",
    "\n",
    "    #with open('SVM.pkl', 'wb') as f:\n",
    "        #pickle.dump(modelSVM, f)\n",
    "\n",
    "    y_pred_svm = modelSVM.predict(v)\n",
    "    a.append(accuracy(y_true, y_pred_svm))\n",
    "    print(i)\n",
    "plt.plot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "17954a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def bagging(X_train, y_train, X_test):\n",
    "    n = 200\n",
    "    models = []\n",
    "    for i in range(n):\n",
    "        indexs=np.random.choice(X_train.shape[0], size = 500)# sample with replacement\n",
    "        Xi=X_train[indexs, :]# Chossing random samples\n",
    "        Yi=y_train[indexs]\n",
    "        # Training for each sample bunch by Decision Tree Classifier\n",
    "        model=DecisionTreeClassifier(max_depth = 1000)\n",
    "        model.fit(Xi,Yi)\n",
    "        models.append(model)\n",
    "        print(i)\n",
    "    pred=np.zeros((X_test.shape[0]))\n",
    "    # predicting with each stored models\n",
    "    for model in models:\n",
    "        pred=pred+model.predict(X_test)\n",
    "    return np.round(pred/n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "deff4fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "p = bagging(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2ba79ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.42653741290519"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = accuracy(y_test, p)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8b716888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "80.80000000000001\n"
     ]
    }
   ],
   "source": [
    "preds = bagging(X_real1, y_real1, v)\n",
    "print(accuracy(y_true, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3003ad7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35216/2999607513.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#spam_ind = np.where(y_train == 1)[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "#spam_ind = np.where(y_train == 1)[0]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2813b551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\91990\\\\Downloads'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c096a62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
